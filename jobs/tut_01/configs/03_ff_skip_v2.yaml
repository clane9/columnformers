model: vision_tut_res_tiny_patch16_128
mlp_ratio: 2.0
moe_experts: 12
moe_conserve: false
attn_bias: false
init_local_attn: false
lr: 3.0e-04
desc: >
  TUT with direct skip connections from earlier layers (and full pos embedding).
  Compared to 02_ff_skip, I added position embedding to the direct skip input to the
  first layer (which I previously forgot by mistake). Nb I also forgot to update the
  description for 02_ff_skip.

dataset: imagenet-100
crop_min_scale: 0.33334
hflip: 0.5
color_jitter: 0.4
epochs: 50
amp: true
wandb: true
figure_interval: 5
