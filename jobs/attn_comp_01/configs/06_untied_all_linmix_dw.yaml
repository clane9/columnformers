model: vision_transformer_tiny_patch16_128
untied: "1,1,1"
attn_bias: true
attn_head_bias: true
attn_mode: linmixing
skip_attn: false
num_heads: 384
lr: 1.0e-04
desc: >
  all untied weights with attn (head) bias and linear mixing attention (w/o softmax) and
  no attention skip connection and depthwise mixing (num_heads = dim)

dataset: imagenet-100
crop_min_scale: 0.33334
hflip: 0.5
color_jitter: 0.4
epochs: 50
amp: true
wandb: true
figure_interval: 5
