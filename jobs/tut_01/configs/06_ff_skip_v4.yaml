model: vision_tut_ff_tiny_patch16_128
mlp_ratio: 2.0
moe_experts: 10
moe_conserve: false
time_embed: true
attn_bias: false
init_local_attn: false
batch_size: 192
lr: 2.25e-04
desc: >
  Same as 05_t_embed but now the direct input feeds into the attention module, rather
  than skipping it. Also reduced lr and batch size to be consistent with next runs.

dataset: imagenet-100
crop_min_scale: 0.33334
hflip: 0.5
color_jitter: 0.4
epochs: 50
amp: true
wandb: true
figure_interval: 5
