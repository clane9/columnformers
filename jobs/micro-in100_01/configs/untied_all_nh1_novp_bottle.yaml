model: vision_transformer_tiny_patch16_128
untied: "1,1,1"
num_heads: 1
mlp_ratio: 0.166667
qk_head_dim: 64
no_vp: true
desc: "all untied weights with single head attention and no value or proj and bottleneck mlp"

dataset: micro-imagenet-100
epochs: 10
decay_lr: false
warmup_fraction: 0.0
weight_decay: 0.0
amp: true
wandb: true
figure_interval: 1
