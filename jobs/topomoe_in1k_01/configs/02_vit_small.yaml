model: vit_small_patch16_224

dataset: folder/imagenet
# todo: update to your local imagenet folder
# should have train/ and val/ subdirectories
data_dir: /ocean/datasets/community/imagenet
val_split: val
num_classes: 1000

scale: [0.08, 1.0]
hflip: 0.5
color_jitter: 0.4
workers: 10

# recipe adapted from deit, but shorter schedule
# https://github.com/facebookresearch/deit/blob/main/README_deit.md
epochs: 100
# this is the batch size per replica
# assuming 2 32GB gpus
# Nb, got OOM on batch_size 384, so using 256
batch_size: 256
# this is effective lr, i.e. no linear scaling rule
lr: 4.0e-4
# 5 warmup epochs
warmup_fraction: 0.05
min_lr_fraction: 0.01
weight_decay: 0.05

checkpoint_interval: 1
figure_interval: 1

amp: true
wandb: true
debug: false
