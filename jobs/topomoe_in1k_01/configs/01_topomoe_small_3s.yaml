model: topomoe_small_3s_patch16_224
mlp_conserve: true
static_pool: true
wiring_lambd: 0.01

dataset: folder/imagenet
# todo: update to your local imagenet folder
# should have train/ and val/ subdirectories
data_dir: /ocean/datasets/community/imagenet
val_split: val
num_classes: 1000

scale: [0.08, 1.0]
hflip: 0.5
color_jitter: 0.4
workers: 10

# recipe adapted from deit, but shorter schedule
# https://github.com/facebookresearch/deit/blob/main/README_deit.md
epochs: 100
# this is the batch size per replica
# total effective batch size = 1024
# estimate this should need ~60GB
batch_size: 256
# this is effective lr, i.e. no linear scaling rule
lr: 1.0e-3
# 5 warmup epochs
warmup_fraction: 0.05
min_lr_fraction: 0.01
weight_decay: 0.05

amp: true
wandb: true
debug: false
