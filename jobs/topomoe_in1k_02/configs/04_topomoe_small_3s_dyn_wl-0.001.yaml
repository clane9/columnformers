model: topomoe_small_3s_patch16_224
mlp_conserve: true
static_pool: false
wiring_lambd: 0.001

dataset: folder/imagenet
# update to your local imagenet folder
# should have train/ and val/ subdirectories
data_dir: /weka/proj-medarc/shared/imagenet
val_split: val
num_classes: 1000

scale: [0.08, 1.0]
hflip: 0.5
color_jitter: 0.4
workers: 10

# recipe adapted from deit, but shorter schedule
# https://github.com/facebookresearch/deit/blob/main/README_deit.md
epochs: 100
# this is the batch size per gpu
# assuming one gpu
batch_size: 768
# this is effective lr, i.e. no linear scaling rule
lr: 8.0e-4
# 5 warmup epochs
warmup_fraction: 0.05
min_lr_fraction: 0.01
weight_decay: 0.05

checkpoint_interval: 1
figure_interval: 1

amp: true
wandb: true
debug: false
